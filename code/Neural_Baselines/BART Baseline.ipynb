{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb5936e-57b6-4e31-837f-12624dec99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d760fbc-ddd1-4ae6-8d2a-d1a452c5d2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu102'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f50f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "989d89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.callbacks.ModelCheckpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9f09904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_CHKPT = 'facebook/bart-large-xsum'\n",
    "DATA_SRCS   = {'train': '../../data/data_clean/polisumm_fs_train.csv', \n",
    "               'test': '../../data/data_clean/polisumm_fs_test.csv', \n",
    "               'val': '../../data/data_clean/polisumm_fs_val.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c0462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(BART_CHKPT, forced_bos_token_id = 0)\n",
    "tokenizer = BartTokenizer.from_pretrained(BART_CHKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10e246",
   "metadata": {},
   "source": [
    "# Dataset Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f50d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=1024, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "    ''' Function that tokenizes a sentence \n",
    "      Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences\n",
    "      Returns: Dictionary with keys: input_ids, attention_mask, target_ids\n",
    "    '''\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    target_ids = []\n",
    "    tokenized_sentences = {}\n",
    "\n",
    "    for sentence in source_sentences:\n",
    "        encoded_dict = tokenizer(\n",
    "              sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "          )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim = 0)\n",
    "    attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    for sentence in target_sentences:\n",
    "        encoded_dict = tokenizer(\n",
    "              sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "          )\n",
    "        # Shift the target ids to the right\n",
    "        # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "        target_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    target_ids = torch.cat(target_ids, dim = 0)\n",
    "\n",
    "\n",
    "    batch = {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      \"labels\": target_ids,\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12bba0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        \n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "165c1a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummEvalModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, src_csv, batch_size = 64):\n",
    "        '''\n",
    "            Data expected to have columns ('all_texts', 'all_sum')\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_csv = src_csv\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.data = pd.read_csv(self.src_csv)\n",
    "        assert 'all_texts' in self.data.columns, 'Missing source texts column: all_texts'\n",
    "        assert 'all_sum' in self.data.columns, 'Missing target summaries column: all_sum'\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "#         self.test_encodings = self.tokenizer(self.data['all_texts'].astype(str).values, \n",
    "#                                                 self.data['all_sum'].astype(str).values,\n",
    "#                                                 truncation = True, padding = True,\n",
    "#                                                 return_tensors = 'pt')\n",
    "        src_texts = self.data['all_texts'].astype(str).values\n",
    "        targ_texts = self.data['all_sum'].astype(str).values\n",
    "        \n",
    "        self.test_encodings = encode_sentences(self.tokenizer, src_texts, targ_texts, return_tensors = 'pt')\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.test_encodings)\n",
    "        test_dl = torch.utils.data.DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "78b1ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, src_csvs, batch_size = 64):\n",
    "        '''\n",
    "            Data expected to have columns ('all_texts', 'all_sum')\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_csvs = src_csvs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.data_dict = {key: pd.read_csv(val) for key, val in self.src_csvs.items()}\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        train_src, train_targ = self.df_to_pairs(self.data_dict['train'])\n",
    "        test_src, test_targ   = self.df_to_pairs(self.data_dict['test'])\n",
    "        val_src, val_targ     = self.df_to_pairs(self.data_dict['val'])\n",
    "        \n",
    "        self.train_encodings = encode_sentences(self.tokenizer, train_src, train_targ, return_tensors = 'pt')\n",
    "        self.test_encodings  = encode_sentences(self.tokenizer, test_src, test_targ, return_tensors = 'pt')\n",
    "        self.val_encodings   = encode_sentences(self.tokenizer, val_src, val_targ, return_tensors = 'pt')\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.train_encodings)\n",
    "        train_dl = DataLoader(dataset, \n",
    "                              batch_size = self.batch_size, \n",
    "                              shuffle = True)\n",
    "        return train_dl\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.test_encodings)\n",
    "        test_dl = DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "        return test_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.val_encodings)\n",
    "        val_dl = DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def df_to_pairs(self, df):\n",
    "        src_l = 'summarize left: ' + df['all_texts'].astype(str)\n",
    "        src_r = 'summarize right: ' + df['all_texts'].astype(str)\n",
    "        src_texts_double = np.concatenate((src_l.values, src_r.values))\n",
    "        \n",
    "        targ_l    = df['left_sum'].astype(str).values\n",
    "        targ_r    = df['right_sum'].astype(str).values\n",
    "        targs     = np.concatenate((targ_l, targ_r))\n",
    "        \n",
    "        return src_texts_double, targs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add5eb59",
   "metadata": {},
   "source": [
    "# Lightning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "75aae551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, model, test_in_train = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.test_in_train = test_in_train\n",
    "        \n",
    "        if self.test_in_train:\n",
    "            self.rouge_scorer = ROUGEScore()\n",
    "            self.bert_scorer  = BERTScore()\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        train_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        return {'loss': train_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        res_dict = {'loss': val_loss}\n",
    "        \n",
    "        # Summarization Metrics\n",
    "        if self.test_in_train:\n",
    "            gen_summs = self.generate_summ(batch, max_len = 256)\n",
    "            ref_summs = self.untokenize_targ(batch)\n",
    "            \n",
    "            # ROUGE Score\n",
    "            rouge_keep = ('rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure')\n",
    "            rouge_scores = self.rouge_scorer(gen_summs, ref_summs)\n",
    "            rouge_scores = {name:score for name, score in rouge_scores if name in rouge_keep}          \n",
    "            \n",
    "            res_dict.update(rouge_scores)\n",
    "        \n",
    "        return res_dict\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        test_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        res_dict = {'loss': test_loss}\n",
    "        gen_summs = self.generate_summ(batch, max_len = 256)\n",
    "        ref_summs = self.untokenize_targ(batch)\n",
    "        \n",
    "        # ROUGE Score\n",
    "        rouge_keep = ('rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure')\n",
    "        rouge_scores = self.rouge_scorer(gen_summs, ref_summs)\n",
    "        rouge_scores = {name:score for name, score in rouge_scores if name in rouge_keep}          \n",
    "\n",
    "        res_dict.update(rouge_scores)\n",
    "        \n",
    "        # BERT Score\n",
    "        bert_scores = self.bert_scorer(gen_summs, ref_summs)\n",
    "        bert_scores = {'bert_' + key:val for key, val in bert_scores.items()}\n",
    "        res_dict.update(bert_scores)\n",
    "        \n",
    "        return res_dict\n",
    "    \n",
    "    def generate_summ(self, sample, eval_beams = 3, early_stopping = True, max_len = 40):\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            sample[\"input_ids\"],\n",
    "            attention_mask=sample[\"attention_mask\"],\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id = self.tokenizer.pad_token_id,\n",
    "            num_beams= eval_beams,\n",
    "            max_length = max_len,\n",
    "            early_stopping = early_stopping\n",
    "        )\n",
    "        \n",
    "        gen_summ = [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
    "        return gen_summ\n",
    "    \n",
    "    def untokenize_targ(self, sample):\n",
    "        sent      = sample['labels']\n",
    "        targ_summ = [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in sent]\n",
    "        return targ_summ\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b1fc1",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3892cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = PoliSummModel(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "80b35089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod = PoliSummDataModule(tokenizer, DATA_SRCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0019cd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "02359896",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4733c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = data_mod.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f8217d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus = 0, log_every_n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9fc971ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Trainer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2c1b50b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type                         | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model        | BartForConditionalGeneration | 406 M \n",
      "1 | rouge_scorer | ROUGEScore                   | 0     \n",
      "--------------------------------------------------------------\n",
      "406 M     Trainable params\n",
      "0         Non-trainable params\n",
      "406 M     Total params\n",
      "1,625.162 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndeas\\anaconda3\\envs\\abstractcos\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:489: UserWarning: One of given dataloaders is None and it will be skipped.\n",
      "  rank_zero_warn(\"One of given dataloaders is None and it will be skipped.\")\n",
      "C:\\Users\\ndeas\\anaconda3\\envs\\abstractcos\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\Users\\ndeas\\anaconda3\\envs\\abstractcos\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:433: UserWarning: The number of training samples (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                   | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndeas\\anaconda3\\envs\\abstractcos\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(lightning_model, datamodule = data_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b6a4e87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndeas\\anaconda3\\envs\\abstractcos\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:   0%|                                                                                | 0/12 [2:47:35<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer.test(lightning_model, data_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45abed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abs_cos",
   "language": "python",
   "name": "abs_cos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
