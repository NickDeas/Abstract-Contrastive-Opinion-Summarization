{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bb95e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from ast import literal_eval\n",
    "import json\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "367c7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_BASE      = '../../results/'\n",
    "\n",
    "BART_ZS_FP    = RES_BASE + 'bart_zshot_gens.csv'\n",
    "PRIMERA_ZS_FP = RES_BASE + 'primera_zshot_gens.csv'\n",
    "\n",
    "LR_RES_FP     = RES_BASE + 'lexrank_results.csv'\n",
    "CMOS_RES_FP   = RES_BASE + 'cmos_results.csv'\n",
    "TOGL_RES_FP   = RES_BASE + '/togl_decoding/togl_predictions.csv'\n",
    "COCO_RES_FP   = RES_BASE + 'cocosum_results.csv'\n",
    "BART_FS_FP    = RES_BASE + 'bart_fshot_gens.csv'\n",
    "PRIMERA_FS_FP = RES_BASE + 'primera_fshot_gens.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2f7e7",
   "metadata": {},
   "source": [
    "# Data Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "4daf7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive Baselines\n",
    "lr_res = pd.read_csv(LR_RES_FP)\n",
    "cmos_res = pd.read_csv(CMOS_RES_FP)\n",
    "\n",
    "# Abstractive Pre-trained Baselines\n",
    "bart_zs_res = pd.read_csv(BART_ZS_FP)\n",
    "bart_fs_res = pd.read_csv(BART_FS_FP)\n",
    "prim_zs_res = pd.read_csv(PRIMERA_ZS_FP)\n",
    "prim_fs_res = pd.read_csv(PRIMERA_FS_FP)\n",
    "\n",
    "# Abstract Contrastive Approaches\n",
    "coco_res = pd.read_csv(COCO_RES_FP)\n",
    "togl_res = pd.read_csv(TOGL_RES_FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "26252b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_res['title_date'] = lr_res['title'] + '_' + lr_res['date']\n",
    "lr_res = lr_res[['title_date', 'lexrank_lsum', 'lexrank_rsum', 'left_sum', 'right_sum']]\n",
    "cmos_res['title_date'] = cmos_res['title'] + '_' + cmos_res['date']\n",
    "cmos_res = cmos_res[['title_date', 'cmos_lsum', 'cmos_rsum', 'left_sum', 'right_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "5a5bdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "togl_res.columns = ['title_date', 'togl_lsum', 'togl_rsum']\n",
    "togl_res = togl_res.merge(lr_res[['title_date', 'left_sum', 'right_sum']], on = 'title_date')\n",
    "\n",
    "bart_fs_res = bart_fs_res.merge(lr_res[['title_date', 'left_sum', 'right_sum']], on = 'title_date')\n",
    "prim_fs_res = prim_fs_res.merge(lr_res[['title_date', 'left_sum', 'right_sum']], on = 'title_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526638c",
   "metadata": {},
   "source": [
    "# Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "7de01bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer = True)\n",
    "\n",
    "bert_scorer = BERTScore(model_name_or_path = 'roberta-large-mnli', \n",
    "                        rescale_with_baseline = True, \n",
    "                        lang = 'en',\n",
    "                        verbose = True, \n",
    "                        max_length = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13750e5d",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "b6cb7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge(text, ref):\n",
    "    '''\n",
    "        Get Rouge-F1 scores given a text and reference\n",
    "    '''\n",
    "    \n",
    "    rouge = scorer.score(text, ref)\n",
    "    rouge1 = rouge['rouge1'].fmeasure\n",
    "    rouge2 = rouge['rouge2'].fmeasure\n",
    "    rougeL = rouge['rougeL'].fmeasure\n",
    "    \n",
    "    return rouge1, rouge2, rougeL\n",
    "\n",
    "def rouge_and_reverse(row, model):\n",
    "    '''\n",
    "        Calculate rouge scores given a row from a results dataset.\n",
    "        Also swap the left and right text for approaches that are not expected to distinguish\n",
    "            left and right perspectives.\n",
    "    '''\n",
    "    \n",
    "    lsum, rsum   = row[f'left_sum'], row['right_sum']\n",
    "    ltext, rtext = row[f'{model}_lsum'], row[f'{model}_rsum']\n",
    "    \n",
    "    # Rouge Scores\n",
    "    lrouge_1f, lrouge_2f, lrouge_Lf = get_rouge(ltext, lsum)\n",
    "    rrouge_1f, rrouge_2f, rrouge_Lf = get_rouge(rtext, rsum)\n",
    "    \n",
    "    lrouge_1r, lrouge_2r, lrouge_Lr = get_rouge(ltext, rsum)\n",
    "    rrouge_1r, rrouge_2r, rrouge_Lr = get_rouge(rtext, lsum)\n",
    "    \n",
    "    lrouge_1, lrouge_2, lrouge_L = lrouge_1f, lrouge_2f, lrouge_Lf\n",
    "    rrouge_1, rrouge_2, rrouge_L = rrouge_1f, rrouge_2f, rrouge_Lf\n",
    "    reverse = False\n",
    "    \n",
    "    # If the reverse scoring is better than the forward scoring, swap left and right texts\n",
    "    if (lrouge_2f + rrouge_2f) < (lrouge_2r + rrouge_2r):\n",
    "        temp = ltext\n",
    "        ltext = rtext\n",
    "        rtext = temp\n",
    "        \n",
    "        lrouge_1, lrouge_2, lrouge_L = lrouge_1r, lrouge_2r, lrouge_Lr\n",
    "        rrouge_1, rrouge_2, rrouge_L = rrouge_1r, rrouge_2r, rrouge_Lr\n",
    "        \n",
    "        reverse = True\n",
    "    \n",
    "    row['new_l_text'] = ltext\n",
    "    row['new_r_text'] = rtext\n",
    "    row['l_rouge1'] = lrouge_1\n",
    "    row['l_rouge2'] = lrouge_2\n",
    "    row['l_rougeL'] = lrouge_L\n",
    "    row['r_rouge1'] = rrouge_1\n",
    "    row['r_rouge2'] = rrouge_2\n",
    "    row['r_rougeL'] = rrouge_L\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "387f5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_bleus(preds1, preds2):\n",
    "    self_bleus = []\n",
    "    for pred1, pred2 in tqdm(zip(preds1, preds2)):\n",
    "        bleu = bleu_score.sentence_bleu(pred1, pred2, weights = (1.0,))\n",
    "        self_bleus.append(bleu)\n",
    "    return self_bleus\n",
    "\n",
    "def get_self_bleus_df(df, model):\n",
    "    ltexts = df[f'{model}_lsum'].values\n",
    "    rtexts = df[f'{model}_rsum'].values\n",
    "    \n",
    "    self_bleus = get_self_bleus(ltexts, rtexts)\n",
    "    \n",
    "    return self_bleus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "3b40fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_scores_df(df, model):\n",
    "    l_bscores = bert_scorer(df['new_l_text'].astype(str).tolist()[0:1], df['left_sum'].astype(str).tolist()[0:1])\n",
    "    r_bscores = bert_scorer(df['new_r_text'].astype(str).tolist()[0:1], df['right_sum'].astype(str).tolist()[0:1])\n",
    "    \n",
    "    df['l_bscore'] = l_bscores['f1']\n",
    "    df['r_bscore'] = r_bscores['f1']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d9be2",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128edcae",
   "metadata": {},
   "source": [
    "##  Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4831d7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8fc5413",
   "metadata": {},
   "source": [
    "## Experiment 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "8d44b659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 735/735 [00:04<00:00, 148.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 735/735 [00:03<00:00, 192.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 735/735 [00:04<00:00, 168.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 735/735 [00:07<00:00, 104.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 735/735 [00:04<00:00, 156.40it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 735/735 [00:12<00:00, 58.96it/s]\n"
     ]
    }
   ],
   "source": [
    "lr_res      = lr_res.progress_apply(lambda row: rouge_and_reverse(row, 'lexrank'), axis = 1)\n",
    "cmos_res    = cmos_res.progress_apply(lambda row: rouge_and_reverse(row, 'cmos'), axis = 1)\n",
    "\n",
    "bart_fs_res = bart_fs_res.progress_apply(lambda row: rouge_and_reverse(row, 'bart'), axis = 1)\n",
    "prim_fs_res = prim_fs_res.progress_apply(lambda row: rouge_and_reverse(row, 'primera'), axis = 1)\n",
    "\n",
    "togl_res    = togl_res.progress_apply(lambda row: rouge_and_reverse(row, 'togl'), axis = 1)\n",
    "coco_res    = coco_res.progress_apply(lambda row: rouge_and_reverse(row, 'coco'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "7e268892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "735it [00:03, 229.42it/s]\n",
      "735it [00:01, 730.91it/s]\n",
      "735it [00:01, 525.71it/s]\n",
      "735it [00:15, 47.66it/s]\n",
      "735it [00:01, 458.65it/s]\n",
      "735it [00:03, 202.68it/s]\n"
     ]
    }
   ],
   "source": [
    "lr_res['self_bleu']      = get_self_bleus_df(lr_res, 'lexrank')\n",
    "cmos_res['self_bleu']    = get_self_bleus_df(cmos_res, 'cmos')\n",
    "\n",
    "bart_fs_res['self_bleu'] = get_self_bleus_df(bart_fs_res, 'bart')\n",
    "prim_fs_res['self_bleu'] = get_self_bleus_df(prim_fs_res, 'primera')\n",
    "\n",
    "togl_res['self_bleu']    = get_self_bleus_df(togl_res, 'togl')\n",
    "coco_res['self_bleu']    = get_self_bleus_df(coco_res, 'coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "e6a87128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac8ef8b86404cc895e2ab7625b0e94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 277, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af1901f5a647beb99826476819455d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x000001F77F962F78>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 277, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaModel: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f001f764b0af41d782fd31b8c87f6383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x000001F77F962F78>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 132, in __del__\n",
      "    self._close()\n",
      "  File \"C:\\Users\\ndeas\\miniconda3\\lib\\multiprocessing\\connection.py\", line 277, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02f18a3d3124223ab7c5b6257ad3365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:05<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (4) does not match length of index (735)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-441-151ab13fcc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlr_res_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_scores_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lexrank'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcmos_res_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_scores_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmos_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cmos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcoco_res_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_scores_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoco_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coco'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtogl_res_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_scores_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtogl_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'togl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbart_fs_res_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_scores_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbart_fs_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bart'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-439-32150212a26d>\u001b[0m in \u001b[0;36mget_bert_scores_df\u001b[1;34m(df, model)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mr_bscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new_r_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'right_sum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'l_bscore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_bscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'r_bscore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_bscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3611\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3612\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m         \"\"\"\n\u001b[1;32m-> 3784\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3785\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m         if (\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4508\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4509\u001b[1;33m             \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4510\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         raise ValueError(\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[1;34m\"does not match length of index \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (4) does not match length of index (735)"
     ]
    }
   ],
   "source": [
    "lr_res_ = get_bert_scores_df(lr_res, 'lexrank')\n",
    "cmos_res_ = get_bert_scores_df(cmos_res, 'cmos')\n",
    "coco_res_ = get_bert_scores_df(coco_res, 'coco')\n",
    "togl_res_ = get_bert_scores_df(togl_res, 'togl')\n",
    "bart_fs_res_ = get_bert_scores_df(bart_fs_res, 'bart')\n",
    "prim_fs_res_ = get_bert_scores_df(prim_fs_res, 'primera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "50841580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_metrics(df):\n",
    "    l_rouge1 = df['l_rouge1'].mean()\n",
    "    l_rouge2 = df['l_rouge2'].mean()\n",
    "    l_rougeL = df['l_rougeL'].mean()\n",
    "    r_rouge1 = df['r_rouge1'].mean()\n",
    "    r_rouge2 = df['r_rouge2'].mean()\n",
    "    r_rougeL = df['r_rougeL'].mean()\n",
    "    \n",
    "    self_bleu = df['self_bleu'].mean()\n",
    "    \n",
    "    return {'l_rouge1': l_rouge1, \n",
    "            'l_rouge2': l_rouge2, \n",
    "            'l_rougeL': l_rougeL,\n",
    "            'l_bscore': l_bscore,\n",
    "            'r_rouge1': r_rouge1, \n",
    "            'r_rouge2': r_rouge2, \n",
    "            'r_rougeL': r_rougeL,\n",
    "            'r_bscore': r_bscore,\n",
    "            'self_bleu': self_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10da6cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1060946720651919,\n",
       " 0.1060946720651919,\n",
       " 0.0811671010399922,\n",
       " 0.14589109178997778,\n",
       " 0.14589109178997778,\n",
       " 0.10925546591557105,\n",
       " 0.12633690320695098)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_metrics(lr_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "98a1c50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14122461171454748,\n",
       " 0.14122461171454748,\n",
       " 0.1155106390652887,\n",
       " 0.13573061115311313,\n",
       " 0.13573061115311313,\n",
       " 0.11021186049868195,\n",
       " 0.26132524432516635)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_metrics(cmos_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c7359680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17772615022316526,\n",
       " 0.17772615022316526,\n",
       " 0.14118185834573072,\n",
       " 0.17889027812804886,\n",
       " 0.17889027812804886,\n",
       " 0.14251107693517712,\n",
       " 0.2720906059832041)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_metrics(togl_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2e20d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
