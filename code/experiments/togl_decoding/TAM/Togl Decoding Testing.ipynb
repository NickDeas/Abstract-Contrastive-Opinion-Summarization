{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a63e1ff2-7b41-413e-8647-0ca759b59e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from transformers import BartTokenizer, LEDTokenizer\n",
    "from transformers import BartForConditionalGeneration\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.generation_utils import top_k_top_p_filtering, BeamSearchScorer\n",
    "from transformers.pytorch_utils import torch_int_div\n",
    "\n",
    "\n",
    "import data_utils, dataset, model_utils, topic_metrics\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2f329f1d-b54c-44a1-a3be-76d267f5d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ff5bb-9367-465c-b236-31bc8fcc9169",
   "metadata": {},
   "source": [
    "# Loading Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "793778a3-18c5-4e69-8bac-b0207de77b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bec26f6f-bed2-4071-a73c-e76588152bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec719b-e5b0-4c8f-a0fe-68335c85ce8e",
   "metadata": {},
   "source": [
    "# Custom Decoding Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "367edfd6-8ce3-4fe3-bb11-8132e8b70c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToGLDecoder:\n",
    "    \n",
    "    def __init__(self,\n",
    "                model: PreTrainedModel,\n",
    "                tokenizer,\n",
    "                top_p: float = 1.0,\n",
    "                togl_func: str = 'sum',\n",
    "                togl_func_kwargs: dict = None,\n",
    "                device = None):\n",
    "        '''\n",
    "            Parameters:\n",
    "                -model: PreTrainedModel\n",
    "                    A Huggingface pretrained model capable of generating text\n",
    "                -top_p: float\n",
    "                    Parameter for top-p sampling decoding method\n",
    "                -togl_func\n",
    "                    Function used to combine model predictions and topic model word distribution.\n",
    "                    Defaults to the sum of the generation and topic model word distributions with weight 1.\n",
    "                -togl_func_kwargs\n",
    "                    Keyword arguments to pass to the togl_func beyond word distribution parameters\n",
    "                -device\n",
    "                    Torch/Cuda device to use while generating\n",
    "        '''\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.top_p = top_p\n",
    "        if type(togl_func) == str:\n",
    "            assert togl_func in ('sum'), f'togl_func {togl_func} has not been implemented'\n",
    "            if togl_func == 'sum':\n",
    "                self.togl_func = self.togl_sum\n",
    "        else:\n",
    "            self.togl_func = togl_func\n",
    "        \n",
    "        self.togl_func_kwargs = togl_func_kwargs\n",
    "        \n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self,\n",
    "                    inputs: torch.Tensor,\n",
    "                    togl_probs: torch.Tensor,\n",
    "                    togl_start: int = 2,\n",
    "                    use_cache: bool = True,\n",
    "                    decoder_start_token_id = None,\n",
    "                    num_beams: int  = 3,\n",
    "                    no_repeat_ngram_size = 3,\n",
    "                    min_length: int = 16,\n",
    "                    max_length: int = 1024,\n",
    "                    early_stopping: bool = True,\n",
    "                    **model_kwargs):\n",
    "        '''\n",
    "            Generates a sequence using beam search sampling incorporating ToGL-Decoding.\n",
    "            \n",
    "            Code drawn from \n",
    "                - https://github.com/huggingface/transformers/blob/v4.25.1/src/transformers/generation/utils.py#L998\n",
    "                    - beam_sample function\n",
    "                - https://github.com/megagonlabs/cocosum/blob/main/decode.py\n",
    "                    - generate_function\n",
    "        '''\n",
    "        \n",
    "        batch_size = 1\n",
    "        \n",
    "        inputs         = inputs.to(self.device)\n",
    "        togl_probs     = togl_probs.to(self.device)\n",
    "            \n",
    "        inputs_t, model_input_name, model_kwargs = self.model._prepare_model_inputs(inputs, self.tokenizer.bos_token_id, model_kwargs)\n",
    "        \n",
    "        model_kwargs = self.model._prepare_encoder_decoder_kwargs_for_generation(\n",
    "            inputs_t, model_kwargs, model_input_name\n",
    "        )\n",
    "        \n",
    "        input_ids = self.model._prepare_decoder_input_ids_for_generation(\n",
    "                batch_size,\n",
    "                decoder_start_token_id=self.tokenizer.bos_token_id,\n",
    "                bos_token_id=self.tokenizer.bos_token_id,\n",
    "                model_kwargs=model_kwargs,\n",
    "                device=self.device,\n",
    "            )\n",
    "        \n",
    "        logits_processor = self.model.model._get_logits_processor(\n",
    "            repetition_penalty = None,\n",
    "            no_repeat_ngram_size = no_repeat_ngram_size,\n",
    "            encoder_no_repeat_ngram_size=None,\n",
    "            input_ids_seq_length = input_ids.shape[-1],\n",
    "            encoder_input_ids = inputs_t,\n",
    "            min_length=min_length,\n",
    "            max_length=max_length,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            forced_bos_token_id=None,\n",
    "            forced_eos_token_id=None,\n",
    "            num_beams=num_beams,\n",
    "            num_beam_groups=None,\n",
    "            diversity_penalty=None,\n",
    "            remove_invalid_values=None,\n",
    "            bad_words_ids = None,\n",
    "            prefix_allowed_tokens_fn = None,\n",
    "            exponential_decay_length_penalty = None,\n",
    "            logits_processor = [],\n",
    "            renormalize_logits = None,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        stopping_criteria = self.model.model._get_stopping_criteria(\n",
    "            max_length = max_length, max_time = None, stopping_criteria = []\n",
    "        )\n",
    "        \n",
    "        # Setup beam scorer for searching generations\n",
    "        beam_scorer = BeamSearchScorer(\n",
    "            batch_size = batch_size,\n",
    "            num_beams = num_beams,\n",
    "            device = self.device,\n",
    "            do_early_stopping = early_stopping,\n",
    "            num_beam_hyps_to_keep = 1\n",
    "        )\n",
    "        \n",
    "        input_ids, model_kwargs = self.model._expand_inputs_for_generation(\n",
    "            input_ids, expand_size=num_beams, is_encoder_decoder=True, **model_kwargs\n",
    "        )\n",
    "        \n",
    "        batch_size = len(beam_scorer._beam_hyps)\n",
    "        \n",
    "        batch_beam_size, cur_len = input_ids.shape\n",
    "        \n",
    "        beam_scores = torch.zeros((batch_size, num_beams), \n",
    "                                  dtype = torch.float, \n",
    "                                  device = self.device)\n",
    "        beam_scores[:, 1:] = -1e-9\n",
    "        beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "        beam_indices = (None)\n",
    "        \n",
    "        \n",
    "        \n",
    "        while True:            \n",
    "        \n",
    "            model_in = self.model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "            outputs = self.model(**model_in, \n",
    "                                 return_dict=True,\n",
    "                                 output_attentions = False,\n",
    "                                 output_hidden_states = False)\n",
    "            \n",
    "            # Update togl_logits to zero selected terms in output\n",
    "            togl_probs[input_ids] = 0. #float('-inf')\n",
    "                        \n",
    "            # Modify logits\n",
    "            raw_logits = outputs.logits\n",
    "            if cur_len >= togl_start:\n",
    "                mod_logits = self.togl_func(raw_logits, togl_probs)\n",
    "            else:\n",
    "                mod_logits = raw_logits\n",
    "            mod_logits = mod_logits[:, -1, :]\n",
    "            \n",
    "            next_logits = self.model.model.adjust_logits_during_generation(mod_logits, cur_len = cur_len)\n",
    "            next_scores = F.log_softmax(next_logits, dim = -1)\n",
    "            \n",
    "            next_scores_pp = logits_processor(input_ids, next_scores)\n",
    "            next_scores = next_scores_pp + beam_scores[:, None].expand_as(next_scores)\n",
    "            \n",
    "            vocab_size  = next_scores.shape[-1]\n",
    "            next_scores = next_scores.view(batch_size, num_beams * vocab_size)\n",
    "            \n",
    "            next_scores, next_tokens = torch.topk(\n",
    "                next_scores, 2 * num_beams, dim = 1, largest = True, sorted = True\n",
    "            )\n",
    "            \n",
    "            # probs = F.softmax(next_scores, dim = -1)\n",
    "            \n",
    "            # next_tokens = torch.multinomial(probs, num_samples = 2*num_beams)\n",
    "            # next_scores = torch.gather(next_scores, -1, next_tokens)\n",
    "            \n",
    "            # next_scores, idxs = torch.sort(next_scores, descending = True, dim = 1)\n",
    "            # next_tokens = torch.gather(next_tokens, -1, idxs)\n",
    "            \n",
    "            next_idxs = torch_int_div(next_tokens, vocab_size)\n",
    "            next_tokens = next_tokens % vocab_size\n",
    "            \n",
    "            beam_outputs = beam_scorer.process(\n",
    "                input_ids,\n",
    "                next_scores,\n",
    "                next_tokens,\n",
    "                next_idxs,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                beam_indices=beam_indices,\n",
    "            )\n",
    "            beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "            beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "            \n",
    "            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim = -1)\n",
    "            \n",
    "            model_kwargs = self.model.model._update_model_kwargs_for_generation(\n",
    "                outputs, model_kwargs, is_encoder_decoder = True\n",
    "            )\n",
    "            \n",
    "            cur_len += 1\n",
    "            \n",
    "            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n",
    "                break\n",
    "            \n",
    "        seq_outputs = beam_scorer.finalize(\n",
    "            input_ids,\n",
    "            beam_scores,\n",
    "            next_tokens,\n",
    "            next_idxs,\n",
    "            pad_token_id = self.tokenizer.pad_token_id,\n",
    "            eos_token_id = self.tokenizer.eos_token_id,\n",
    "            max_length = stopping_criteria.max_length,\n",
    "            beam_indices = beam_indices,\n",
    "        )\n",
    "        \n",
    "        return seq_outputs['sequences']\n",
    "    \n",
    "    def togl_sum(self, raw_out, togl_probs):\n",
    "        norms = raw_out.norm(dim = -1)\n",
    "        togl_probs = togl_probs\n",
    "        togl_probs = togl_probs.unsqueeze(0).repeat((raw_out.shape[0], 1))\n",
    "        mod_logits = ((raw_out.squeeze()/norms) + togl_probs) * norms\n",
    "        mod_logits = mod_logits.unsqueeze(1)\n",
    "        # raise ValueError('debug')\n",
    "        return mod_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "18893067-5a73-42ee-91ba-3c87d7ff065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['The top US hostage affairs official on Sunday reflected on conducting the prisoner swap that led to Brittney Griner’s release, saying the WNBA star immediately thanked the crew returning her to the United States.\\n“When she finally got on to the US plane, I said, ‘Brittney, you must have been through a lot over the last 10 months. Here’s your seat. Please feel free to decompress. We’ll give you your space,’” Special Presidential Envoy for Hostage Affairs Roger Carstens told CNN’s Dana Bash on “State of the Union.”\\n“And she said, ‘Oh no. I’ve been in prison for 10 months now listening to Russian, I want to talk. But first of all, who are these guys?’ And she moved right past me and went to every member on that crew, looked them in the eyes, shook their hands and asked about them and got their names, making a personal connection with them. It was really amazing,” Carstens recalled. “And then later on, on an 18 hour flight, she probably spent 12 hours just talking and we talked about everything under the sun.', 'goodbye my fellow compatriate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "464401c5-5b0d-404b-93c3-1976479bdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_inputs = bart_tokenizer(inputs, padding = True, return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "9c794339-ac5b-4143-8fa7-4436599add76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_inputs = tok_inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "5e9e4e57-0cfd-454f-bb61-ac747dba353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(tok_inputs['input_ids'], min_length = 10, max_length = 1024, \n",
    "                        num_beams = 3, no_repeat_ngram_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "7b7fb6d9-806a-4d83-b036-9ec41d3e0da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 45])"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "beb6cfd5-47bc-4820-b30b-1bee27a954f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"</s><s>Top US hostage affairs official reflected on the prisoner swap that led to Brittney Griner's release. Roger Carstens said the WNBA star immediately thanked the crew returning her to the U.S.</s><pad>\""
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.decode(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "1940aa6f-8236-4922-8bd5-cfacdefcf4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15536, 5101)"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.get_vocab()['Gr'], bart_tokenizer.get_vocab()['iner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "id": "7de8a534-7a89-4eee-8a95-38f1fdb24376",
   "metadata": {},
   "outputs": [],
   "source": [
    "togl_logits = 0 * torch.ones(50264).to(device)\n",
    "togl_logits[456] = 0.04\n",
    "# togl_logits[5101]  = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "ea1e86a9-cdb8-4af2-bcf4-b91a319c0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ToGLDecoder(model, bart_tokenizer,\n",
    "                     device = device\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "id": "f57af7cb-bc5c-4baf-9bf6-3c9fce11c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = decoder.generate(tok_inputs['input_ids'], \n",
    "                          togl_logits, togl_start = 10,\n",
    "                          min_length = 2, max_length = 20,\n",
    "                          no_repeat_ngram_size = 3,\n",
    "                          num_beams = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "2230886d-7bed-452b-8ba2-873f0e291ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><s><s>Brittney Griner thanked the crew again after her release from prison, Roger</s>'"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3043439b-d57b-4e27-b844-d0157a24d2db",
   "metadata": {},
   "source": [
    "Ġ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57656954-056a-47e1-a99d-fad019b83c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
