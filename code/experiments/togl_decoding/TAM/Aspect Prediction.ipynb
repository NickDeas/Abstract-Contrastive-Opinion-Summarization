{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b41720c",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecd4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "from tam_lib import data_utils, modeling, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45163a56",
   "metadata": {},
   "source": [
    "# Settings and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10953f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLISUM_SRC = '../../../../data/polisum_clean.csv'\n",
    "CLEAN_OUTPUT = '../../../../data/polisum_tam'\n",
    "TAM_SAVE_PATH = '../../../../models/tam_model'\n",
    "TOGL_DIST_OUT = '../../../../results/togl_decoding/'\n",
    "\n",
    "TEXT_COL        = 'sm_text'\n",
    "CLEAN_TEXT_COL  = 'text_clean'\n",
    "SENT_SPLIT_TOK  = '|||'\n",
    "SENT_RSPLIT_TOK = '\\|\\|\\|'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979b114",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca252ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data files\n",
      "Reading vectorizer\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, vectorizer = data_utils.load_data(CLEAN_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd93b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "polisum = pd.read_csv(POLISUM_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5454b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data_utils.DocDataset(pd.concat((data_train, data_val), axis = 0), \n",
    "                              text_col = CLEAN_TEXT_COL, \n",
    "                              vectorizer = vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9ed8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
    "VECT_VOCAB = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f23122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(train_ds, batch_size = 32, num_workers = NUM_WORKERS, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95ca25",
   "metadata": {},
   "source": [
    "# Loading Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44146f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Topic-Aspect Model\n",
    "tam = modeling.TAM.from_pretrained(TAM_SAVE_PATH, device = DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c14d86",
   "metadata": {},
   "source": [
    "# Predicting Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655c5b2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished batch 1895\r"
     ]
    }
   ],
   "source": [
    "# Run the Topic-Aspect Model to predict distributions for each source sentence\n",
    "asps = {'asp1': None, 'asp2': None, 'title_date': []}\n",
    "\n",
    "i = 0\n",
    "for batch in dl:\n",
    "    bow = batch['bow'].to(DEVICE).squeeze()\n",
    "    asp1, asp2 = tam.pred_aspect_dists(bow)\n",
    "    \n",
    "    if asps['asp1'] is not None:\n",
    "        asps['asp1'] = torch.concat((asps['asp1'], asp1.cpu().detach()), axis = 0)\n",
    "        asps['asp2'] = torch.concat((asps['asp2'], asp2.cpu().detach()), axis = 0)\n",
    "    else:\n",
    "        asps['asp1'] = asp1.cpu().detach()\n",
    "        asps['asp2'] = asp2.cpu().detach()\n",
    "    asps['title_date'].append(batch['title_date'])\n",
    "    print(f'Finished batch {i}', end = '\\r')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb131a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also store the title and dates concatenated to ensure data integrity for evaluation\n",
    "asps['title_date'] = [title for title_l in asps['title_date'] for title in title_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d17d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the indices of title_dates to calculate average distributions\n",
    "title_ix = {}\n",
    "for i, title in enumerate(asps['title_date']):\n",
    "    if title in title_ix.keys():\n",
    "        title_ix[title].append(i)\n",
    "    else:\n",
    "        title_ix[title] = [i]\n",
    "title_ix = {key: np.array(val) for key,val in title_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6816f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished asps 1\n",
      "Finished asps 2\n"
     ]
    }
   ],
   "source": [
    "# Sum and softmax the distributions belonging to the same source id for both aspects\n",
    "# Also extract only the top 10 terms to reduce noisiness\n",
    "asps_1 = {title_date: (asps['asp1'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).values[:10], \n",
    "                       asps['asp1'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).indices[:10]) for title_date, idxs in title_ix.items()}\n",
    "print('Finished asps 1')\n",
    "\n",
    "asps_2 = {title_date: (asps['asp2'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).values[:10], \n",
    "                       asps['asp2'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).indices[:10]) for title_date, idxs in title_ix.items()}\n",
    "print('Finished asps 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7e03af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert top probabilities to lists\n",
    "asps_1 = {td: (probs[0].numpy().tolist(), probs[1].numpy().tolist()) for td, probs in asps_1.items()}\n",
    "asps_2 = {td: (probs[0].numpy().tolist(), probs[1].numpy().tolist()) for td, probs in asps_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46d9faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the left and right aspect distributions\n",
    "with open(TOGL_DIST_OUT + 'asp1_top10.json', 'w') as f:\n",
    "    json.dump(asps_1, f)\n",
    "with open(TOGL_DIST_OUT + 'asp2_top10.json', 'w') as f:\n",
    "    json.dump(asps_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc42f1d",
   "metadata": {},
   "source": [
    "# Vocabulary Mapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366376b",
   "metadata": {},
   "source": [
    "As the Topic-Aspect Model uses a CountVectorizer, and the vocabulary is smaller than language model vocabularies, a mapping between the two lexicons must be created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8e0cefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bart tokenizer and vocabulary\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')\n",
    "bart_vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "901e6734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 9999 of 10000\r"
     ]
    }
   ],
   "source": [
    "# For each term in the Topic-Aspect model vocabulary\n",
    "#    Find the three equivalent tokens in the bart vocabulary \n",
    "#    including the original term, term preceded by a space (Ġ), \n",
    "#    and the term with an uppercase first letter\n",
    "vocab_map = {}\n",
    "MIN_IDX = 3 #minimum length of a subword to search for\n",
    "\n",
    "for i, (term, idx) in enumerate(VECT_VOCAB.items()):\n",
    "    for tok in tokenizer.tokenize(term):\n",
    "        # Create two alternative token representations\n",
    "        tok2 = tok[0].upper() + tok[1:]\n",
    "        tok3 = 'Ġ' + tok\n",
    "\n",
    "        vocab_map[idx] = []\n",
    "        for t in (tok, tok2, tok3):\n",
    "            if t in bart_vocab.keys():\n",
    "                vocab_map[idx].append(bart_vocab[t])\n",
    "            elif t == tok:\n",
    "                # Continually trim the end of the term to search for subword matches\n",
    "                for i in list(range(MIN_IDX, len(t)))[::-1]:\n",
    "                    if t[:i] in bart_vocab.keys():\n",
    "                        vocab_map[idx].append(bart_vocab[t[:i]])\n",
    "                        break\n",
    "                        \n",
    "        # If no matching terms were found, remove the index from the vocab map\n",
    "        if len(vocab_map[idx]) == 0:\n",
    "            del vocab_map[idx]\n",
    "\n",
    "        print(f'Finished {i} of {VOCAB_SIZE}', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "332f6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {int(key): val for key, val in vocab_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4f6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary mapping\n",
    "with open(TOGL_DIST_OUT + 'vocab_map.json', 'w') as f:\n",
    "    json.dump(vocab_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ab91cd",
   "metadata": {},
   "source": [
    "# Finalize Togl Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "81b77138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dist(dist, vocab_map):\n",
    "    '''\n",
    "        Expand distributions into a tuple of a list of all term probabilities and ids\n",
    "            and map the distribution to the bart vocab\n",
    "            \n",
    "        Parameters:\n",
    "            -dist\n",
    "                Pre-processed top-n distribution from Topic-Aspect Model for a set of input documents\n",
    "            -vocab_map\n",
    "                dictionary converting TAM vocab to BART vocab\n",
    "        \n",
    "        Return\n",
    "            Tuple of (term probabilities, term_ids)\n",
    "    '''\n",
    "    # Extract probabilities and token ids\n",
    "    probs = dist[0]\n",
    "    ids = dist[1]\n",
    "    \n",
    "    new_probs, new_ids = [], []\n",
    "    \n",
    "    # Iterate over probs and ids to map to BART vocab including term variants\n",
    "    for prob, idx in zip(probs, ids):\n",
    "        if idx in vocab_map.keys():\n",
    "            new_ids += vocab_map[idx]\n",
    "            new_probs += [prob] * len(vocab_map[idx])\n",
    "    \n",
    "    return (new_probs, new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e8ea1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished asps_1\n",
      "Finished asps_2\n"
     ]
    }
   ],
   "source": [
    "# Map the aspect distributions to BART Vocab\n",
    "asps_1_mapped = {key: expand_dist(val, vocab_map) for key, val in asps_1.items()}\n",
    "print('Finished asps_1')\n",
    "asps_2_mapped = {key: expand_dist(val, vocab_map) for key, val in asps_2.items()}\n",
    "print('Finished asps_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f08c2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the mapped ToGL Distributions for use with ToGL-Decoding\n",
    "with open(TOGL_DIST_OUT + 'asp1_top10_mapped.json', 'w') as f:\n",
    "    json.dump(asps_1_mapped, f)\n",
    "with open(TOGL_DIST_OUT + 'asp2_top10_mapped.json', 'w') as f:\n",
    "    json.dump(asps_2_mapped, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
