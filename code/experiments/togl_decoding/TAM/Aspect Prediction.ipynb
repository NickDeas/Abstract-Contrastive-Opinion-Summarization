{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5b1f15-41f4-48a2-bb90-bcb39cc01964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "from tam_lib import data_utils, modeling, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a12a23-8b3f-4008-a792-1e6d06b6c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLISUM_SRC = '../../../../data/polisum_clean.csv'\n",
    "CLEAN_OUTPUT = '../../../../data/polisum_tam'\n",
    "TAM_SAVE_PATH = '../../../../models/tam_model'\n",
    "TOGL_DIST_OUT = '../../../../results/togl_decoding/'\n",
    "\n",
    "TEXT_COL        = 'sm_text'\n",
    "CLEAN_TEXT_COL  = 'text_clean'\n",
    "SENT_SPLIT_TOK  = '|||'\n",
    "SENT_RSPLIT_TOK = '\\|\\|\\|'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da502d2f-9653-4081-b288-1c12aea0a07a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a919b1-3a87-4764-8c28-44767fe5b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data files\n",
      "Reading vectorizer\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, vectorizer = data_utils.load_data(CLEAN_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13609634-8c72-4ea4-a4ed-b12307d19b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "polisum = pd.read_csv(POLISUM_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5187e8c-cb6b-420e-8e66-ba89b69912c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data_utils.DocDataset(pd.concat((data_train, data_val), axis = 0), \n",
    "                              text_col = CLEAN_TEXT_COL, \n",
    "                              vectorizer = vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8792d1d-d702-4f7b-93c6-9feafaebb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
    "VECT_VOCAB = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b690c9-e0b6-408f-a150-34363f7d09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(train_ds, batch_size = 32, num_workers = NUM_WORKERS, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4fbc99-5b94-4470-be2a-3095b9a60800",
   "metadata": {},
   "source": [
    "# Loading Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05b8809-6d20-4bc9-b47d-084e1c237c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tam = modeling.TAM.from_pretrained(TAM_SAVE_PATH, device = DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77302f19-4a3b-446d-892a-16477e3cfdb7",
   "metadata": {},
   "source": [
    "# Predicting Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1b3851-64c6-4b9d-a5cb-4d48be198719",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished batch 1895\r"
     ]
    }
   ],
   "source": [
    "asps = {'asp1': None, 'asp2': None, 'title_date': []}\n",
    "\n",
    "i = 0\n",
    "for batch in dl:\n",
    "    bow = batch['bow'].to(DEVICE).squeeze()\n",
    "    asp1, asp2 = tam.pred_aspect_dists(bow)\n",
    "    \n",
    "    if asps['asp1'] is not None:\n",
    "        asps['asp1'] = torch.concat((asps['asp1'], asp1.cpu().detach()), axis = 0)\n",
    "        asps['asp2'] = torch.concat((asps['asp2'], asp2.cpu().detach()), axis = 0)\n",
    "    else:\n",
    "        asps['asp1'] = asp1.cpu().detach()\n",
    "        asps['asp2'] = asp2.cpu().detach()\n",
    "    asps['title_date'].append(batch['title_date'])\n",
    "    print(f'Finished batch {i}', end = '\\r')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b028046f-85f3-4c2f-b00a-d53117d58be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "asps['title_date'] = [title for title_l in asps['title_date'] for title in title_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35c127f6-a3a8-4a17-876f-f413e3d406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_ix = {}\n",
    "for i, title in enumerate(asps['title_date']):\n",
    "    if title in title_ix.keys():\n",
    "        title_ix[title].append(i)\n",
    "    else:\n",
    "        title_ix[title] = [i]\n",
    "title_ix = {key: np.array(val) for key,val in title_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "be56035d-4315-4e48-8fa2-386a89d3f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished asps 1\n",
      "Finished asps 2\n"
     ]
    }
   ],
   "source": [
    "asps_1 = {title_date: (asps['asp1'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).values[:10], \n",
    "                       asps['asp1'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).indices[:10]) for title_date, idxs in title_ix.items()}\n",
    "print('Finished asps 1')\n",
    "asps_2 = {title_date: (asps['asp2'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).values[:10], \n",
    "                       asps['asp2'][idxs].sum(dim = 0).softmax(dim = -1).sort(descending = True).indices[:10]) for title_date, idxs in title_ix.items()}\n",
    "print('Finished asps 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec4f190a-2b36-419f-8a7c-fba37ffbd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "asps_1 = {td: (probs[0].numpy().tolist(), probs[1].numpy().tolist()) for td, probs in asps_1.items()}\n",
    "asps_2 = {td: (probs[0].numpy().tolist(), probs[1].numpy().tolist()) for td, probs in asps_2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b7a20c4-42f5-43b8-a682-fc946816b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOGL_DIST_OUT + 'asp1_top10.json', 'w') as f:\n",
    "    json.dump(asps_1, f)\n",
    "with open(TOGL_DIST_OUT + 'asp2_top10.json', 'w') as f:\n",
    "    json.dump(asps_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba119b3-3da1-4cb6-aa22-ce68de332e56",
   "metadata": {},
   "source": [
    "# Vocabulary Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf9abd8d-9926-4d3f-b353-6d072e0c4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24e125b0-0e70-4095-b216-22df62e6f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b9e5f4c4-7313-4dc0-ae3c-f9efff5f59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "46082f86-f31b-4d71-bc92-60ca20107a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 9999 of 10000\r"
     ]
    }
   ],
   "source": [
    "for i, (term, idx) in enumerate(VECT_VOCAB.items()):\n",
    "    for tok in tokenizer.tokenize(term):\n",
    "        tok2 = tok[0].upper() + tok[1:]\n",
    "        tok3 = 'Ä ' + tok\n",
    "\n",
    "        vocab_map[idx] = []\n",
    "        for t in (tok, tok2, tok3):\n",
    "            if t in bart_vocab.keys():\n",
    "                vocab_map[idx].append(bart_vocab[t])\n",
    "        if len(vocab_map[idx]) == 0:\n",
    "            del vocab_map[idx]\n",
    "\n",
    "        print(f'Finished {i} of {VOCAB_SIZE}', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e6012a3-4e96-4703-a529-c8d16d3570b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_map = {int(key): val for key, val in vocab_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d299ba94-61de-4f42-b58b-e99e2093e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOGL_DIST_OUT + 'vocab_map.json', 'w') as f:\n",
    "    json.dump(vocab_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecacece4-8a49-480a-9d1c-56eff7a2762f",
   "metadata": {},
   "source": [
    "# Finalize Togl Distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9b47534e-9285-4e9d-aa31-4c79a9040229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dist(dist, vocab_map):\n",
    "    probs = dist[0]\n",
    "    ids = dist[1]\n",
    "    \n",
    "    new_probs, new_ids = [], []\n",
    "    \n",
    "    for prob, idx in zip(probs, ids):\n",
    "        if idx in vocab_map.keys():\n",
    "            new_ids += vocab_map[idx]\n",
    "            new_probs += [prob] * len(vocab_map[idx])\n",
    "    \n",
    "    return (new_probs, new_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e80b5661-7521-4e47-88c8-361a157e5691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished asps_1\n",
      "Finished asps_2\n"
     ]
    }
   ],
   "source": [
    "asps_1_mapped = {key: expand_dist(val, vocab_map) for key, val in asps_1.items()}\n",
    "print('Finished asps_1')\n",
    "asps_2_mapped = {key: expand_dist(val, vocab_map) for key, val in asps_2.items()}\n",
    "print('Finished asps_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ef3be8f3-57f7-4ded-95e8-ffe57e439518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOGL_DIST_OUT + 'asp1_top10_mapped.json', 'w') as f:\n",
    "    json.dump(asps_1_mapped, f)\n",
    "with open(TOGL_DIST_OUT + 'asp2_top10_mapped.json', 'w') as f:\n",
    "    json.dump(asps_2_mapped, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e8f41-ee52-4331-8fe1-fa56b9ef7889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
