{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9fff6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "839b11b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_CHKPT = 'facebook/bart-large-xsum'\n",
    "DATA_SRCS   = {'train': '../../data/data_clean/polisumm_fs_train.csv', \n",
    "               'test': '../../data/data_clean/polisumm_fs_test.csv', \n",
    "               'val': '../../data/data_clean/polisumm_fs_val.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df23eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(BART_CHKPT, forced_bos_token_id = 0)\n",
    "tokenizer = BartTokenizer.from_pretrained(BART_CHKPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86f1b8",
   "metadata": {},
   "source": [
    "# Dataset Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28ae10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=1024, pad_to_max_length=True, return_tensors=\"pt\"):\n",
    "    ''' Function that tokenizes a sentence \n",
    "      Args: tokenizer - the BART tokenizer; source and target sentences are the source and target sentences\n",
    "      Returns: Dictionary with keys: input_ids, attention_mask, target_ids\n",
    "    '''\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    target_ids = []\n",
    "    tokenized_sentences = {}\n",
    "\n",
    "    for sentence in source_sentences:\n",
    "        encoded_dict = tokenizer(\n",
    "              sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "          )\n",
    "\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim = 0)\n",
    "    attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "\n",
    "    for sentence in target_sentences:\n",
    "        encoded_dict = tokenizer(\n",
    "              sentence,\n",
    "              max_length=max_length,\n",
    "              padding=\"max_length\" if pad_to_max_length else None,\n",
    "              truncation=True,\n",
    "              return_tensors=return_tensors,\n",
    "              add_prefix_space = True\n",
    "          )\n",
    "        # Shift the target ids to the right\n",
    "        # shifted_target_ids = shift_tokens_right(encoded_dict['input_ids'], tokenizer.pad_token_id)\n",
    "        target_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    target_ids = torch.cat(target_ids, dim = 0)\n",
    "\n",
    "\n",
    "    batch = {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "      \"labels\": target_ids,\n",
    "    }\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02fbf12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        \n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b82987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummEvalModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, src_csv, batch_size = 64):\n",
    "        '''\n",
    "            Data expected to have columns ('all_texts', 'all_sum')\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_csv = src_csv\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.data = pd.read_csv(self.src_csv)\n",
    "        assert 'all_texts' in self.data.columns, 'Missing source texts column: all_texts'\n",
    "        assert 'all_sum' in self.data.columns, 'Missing target summaries column: all_sum'\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "#         self.test_encodings = self.tokenizer(self.data['all_texts'].astype(str).values, \n",
    "#                                                 self.data['all_sum'].astype(str).values,\n",
    "#                                                 truncation = True, padding = True,\n",
    "#                                                 return_tensors = 'pt')\n",
    "        src_texts = self.data['all_texts'].astype(str).values\n",
    "        targ_texts = self.data['all_sum'].astype(str).values\n",
    "        \n",
    "        self.test_encodings = encode_sentences(self.tokenizer, src_texts, targ_texts, return_tensors = 'pt')\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.test_encodings)\n",
    "        test_dl = torch.utils.data.DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "        return test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6bd3a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummDataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, src_csvs, batch_size = 64):\n",
    "        '''\n",
    "            Data expected to have columns ('all_texts', 'all_sum')\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.src_csvs = src_csvs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.data_dict = {key: pd.read_csv(val) for key, val in self.src_csvs.items()}\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        train_src, train_targ = self.df_to_pairs(self.data_dict['train'])\n",
    "        test_src, test_targ   = self.df_to_pairs(self.data_dict['test'])\n",
    "        val_src, val_targ     = self.df_to_pairs(self.data_dict['val'])\n",
    "        \n",
    "        self.train_encodings = encode_sentences(self.tokenizer, train_src, train_targ, return_tensors = 'pt')\n",
    "        self.test_encodings  = encode_sentences(self.tokenizer, test_src, test_targ, return_tensors = 'pt')\n",
    "        self.val_encodings = encode_sentences(self.tokenizer, val_src, val_targ, return_tensors = 'pt')\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.train_encodings)\n",
    "        train_dl = DataLoader(dataset, \n",
    "                              batch_size = self.batch_size, \n",
    "                              shuffle = True)\n",
    "        return train_dl\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.test_encodings)\n",
    "        test_dl = DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "        return test_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = PoliSummDataset(self.val_encodings)\n",
    "        val_dl = DataLoader(dataset, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def df_to_pairs(self, df):\n",
    "        src_l = 'summarize left: ' + df['all_texts'].astype(str)\n",
    "        src_r = 'summarize right: ' + df['all_texts'].astype(str)\n",
    "        src_texts_double = np.concatenate((src_texts.values, src_texts.values))\n",
    "        \n",
    "        targ_l    = df['left_sum'].astype(str).values\n",
    "        targ_r    = df['right_sum'].astype(str).values\n",
    "        targs     = np.concatenate((targ_l, targ_r))\n",
    "        \n",
    "        return src_texts_double, targs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334827d5",
   "metadata": {},
   "source": [
    "# Lightning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fdd14b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoliSummModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, tokenizer, model, test_in_train = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.test_in_train = test_in_train\n",
    "        \n",
    "        if self.test_in_train:\n",
    "            self.rouge_scorer = ROUGEScore()\n",
    "        \n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "    \n",
    "    def train_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        return {'loss': val_loss}\n",
    "    \n",
    "    def val_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        res_dict = {'loss': val_loss}\n",
    "        \n",
    "        # Summarization Metrics\n",
    "        if self.test_in_train:\n",
    "            gen_summs = self.generate_summ(batch, max_len = 256)\n",
    "            ref_summs = self.untokenize_targ(batch)\n",
    "            \n",
    "            # ROUGE Score\n",
    "            rouge_keep = ('rouge1_fmeasure', 'rouge2_fmeasure', 'rougeL_fmeasure')\n",
    "            rouge_scores = self.rouge_scorer(gen_summs, ref_summs)\n",
    "            rouge_scores = {name:score for name, score in rouge_scores if name in rouge_keep}          \n",
    "            \n",
    "            res_dict.update(rouge_scores)\n",
    "        \n",
    "        return res_dict\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        src, mask, targ = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        \n",
    "        output = self(src, \n",
    "                     attention_mask = mask,\n",
    "                     decoder_input_ids = targ)\n",
    "        logits = output[0]\n",
    "        \n",
    "        ce_loss = torch.nn.CrossEntropyLoss(ignore_index = self.tokenizer.pad_token_id)\n",
    "        val_loss = ce_loss(logits, targ)\n",
    "        \n",
    "        return {'loss': val_loss}\n",
    "    \n",
    "    def generate_summ(self, sample, eval_beams = 3, early_stopping = True, max_len = 40):\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            sample[\"input_ids\"],\n",
    "            attention_mask=sample[\"attention_mask\"],\n",
    "            use_cache=True,\n",
    "            decoder_start_token_id = self.tokenizer.pad_token_id,\n",
    "            num_beams= eval_beams,\n",
    "            max_length = max_len,\n",
    "            early_stopping = early_stopping\n",
    "        )\n",
    "        \n",
    "        gen_summ = [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
    "        return gen_summ\n",
    "    \n",
    "    def untokenize_targ(self, sample):\n",
    "        sent      = sample['labels']\n",
    "        targ_summ = [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in sent]\n",
    "        return targ_summ\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fdfad2",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "48394caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = PoliSummModel(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ea49cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod = PoliSummDataModule(tokenizer, DATA_SRCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bc3bf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fa846299",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fdfe4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = data_mod.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b9990aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   208,  9112,  ...,     1,     1,     1],\n",
      "        [    0,   140, 33727,  ..., 50118, 13082,     2],\n",
      "        [    0,  1205,   640,  ..., 21792, 27324,     2],\n",
      "        ...,\n",
      "        [    0,   849, 22616,  ...,  1265,   467,     2],\n",
      "        [    0,   849,   975,  ...,    50,  5898,     2],\n",
      "        [    0,  2101, 15478,  ...,  1673,    11,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[  0,  20, 314,  ...,   1,   1,   1],\n",
      "        [  0,  20, 314,  ...,   1,   1,   1],\n",
      "        [  0,  20, 314,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  0,  20, 314,  ...,   1,   1,   1],\n",
      "        [  0,  20, 235,  ...,   1,   1,   1],\n",
      "        [  0,  20, 314,  ...,   1,   1,   1]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dl:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40a212fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cdd15357",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dl:\n",
    "    one_samp = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74809ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_samp = {key: val[0:3, :] for key, val in one_samp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "da5abb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  7639, 15895,  ...,    17,    27,     2],\n",
       "         [    0,  5213,  1721,  ...,   831,  3257,     2],\n",
       "         [    0,   195,   538,  ..., 31014,    63,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[  0,  20, 314,  ...,   1,   1,   1],\n",
       "         [  0,  20, 314,  ...,   1,   1,   1],\n",
       "         [  0,  20, 314,  ...,   1,   1,   1]])}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9cc10355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['People are reacting to the news that Treasury Secretary Steven Mnuchin says the first payments under a plan to rescue the US economy from the Coronavirus could be $1,200 per',\n",
       " \"People are rejoicing at the news that US Vice President Joe Biden has reversed President Donald Trump's ban on transgender people serving in the military.\",\n",
       " \"The latest from the world of tech, including a major News Feed change, a plan to break up Big Tech, and Mark Zuckerberg's old blog posts disappearing.\"]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_model.generate_summ(one_samp, eval_beams = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb188353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstractcos",
   "language": "python",
   "name": "abstractcos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
